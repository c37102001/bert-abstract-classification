{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Categories</th>\n",
       "      <th>Created Date</th>\n",
       "      <th>Task 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>D00001</td>\n",
       "      <td>A Brain-Inspired Trust Management Model to Ass...</td>\n",
       "      <td>Rapid popularity of Internet of Things (IoT) a...</td>\n",
       "      <td>Mahmud/Kaiser/Rahman/Rahman/Shabut/Al-Mamun/Hu...</td>\n",
       "      <td>cs.CR/cs.AI/q-bio.NC</td>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>THEORETICAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>D00002</td>\n",
       "      <td>On Efficient Computation of Shortest Dubins Pa...</td>\n",
       "      <td>In this paper, we address the problem of compu...</td>\n",
       "      <td>Sadeghi/Smith</td>\n",
       "      <td>cs.SY/cs.RO/math.OC</td>\n",
       "      <td>2016-09-21</td>\n",
       "      <td>THEORETICAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>D00003</td>\n",
       "      <td>Data-driven Upsampling of Point Clouds</td>\n",
       "      <td>High quality upsampling of sparse 3D point clo...</td>\n",
       "      <td>Zhang/Jiang/Yang/Yamakawa/Shimada/Kara</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>2018-07-07</td>\n",
       "      <td>ENGINEERING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>D00004</td>\n",
       "      <td>Accessibility or Usability of InteractSE? A He...</td>\n",
       "      <td>Internet is the main source of information now...</td>\n",
       "      <td>Aqle/Khowaja/Al-Thani</td>\n",
       "      <td>cs.HC</td>\n",
       "      <td>2018-08-29</td>\n",
       "      <td>EMPIRICAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>D00005</td>\n",
       "      <td>Spatio-Temporal Facial Expression Recognition ...</td>\n",
       "      <td>Automated Facial Expression Recognition (FER) ...</td>\n",
       "      <td>Hasani/Mahoor</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>2017-03-20</td>\n",
       "      <td>ENGINEERING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id                                              Title  \\\n",
       "0  D00001  A Brain-Inspired Trust Management Model to Ass...   \n",
       "1  D00002  On Efficient Computation of Shortest Dubins Pa...   \n",
       "2  D00003             Data-driven Upsampling of Point Clouds   \n",
       "3  D00004  Accessibility or Usability of InteractSE? A He...   \n",
       "4  D00005  Spatio-Temporal Facial Expression Recognition ...   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  Rapid popularity of Internet of Things (IoT) a...   \n",
       "1  In this paper, we address the problem of compu...   \n",
       "2  High quality upsampling of sparse 3D point clo...   \n",
       "3  Internet is the main source of information now...   \n",
       "4  Automated Facial Expression Recognition (FER) ...   \n",
       "\n",
       "                                             Authors            Categories  \\\n",
       "0  Mahmud/Kaiser/Rahman/Rahman/Shabut/Al-Mamun/Hu...  cs.CR/cs.AI/q-bio.NC   \n",
       "1                                      Sadeghi/Smith   cs.SY/cs.RO/math.OC   \n",
       "2             Zhang/Jiang/Yang/Yamakawa/Shimada/Kara                 cs.CV   \n",
       "3                              Aqle/Khowaja/Al-Thani                 cs.HC   \n",
       "4                                      Hasani/Mahoor                 cs.CV   \n",
       "\n",
       "  Created Date       Task 2  \n",
       "0   2018-01-11  THEORETICAL  \n",
       "1   2016-09-21  THEORETICAL  \n",
       "2   2018-07-07  ENGINEERING  \n",
       "3   2018-08-29    EMPIRICAL  \n",
       "4   2017-03-20  ENGINEERING  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('../data/task2_trainset.csv', dtype=str)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop('Title',axis=1,inplace=True)\n",
    "dataset.drop('Categories',axis=1,inplace=True)\n",
    "dataset.drop('Created Date',axis=1, inplace=True)\n",
    "dataset.drop('Authors',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Task 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>D00001</td>\n",
       "      <td>Rapid popularity of Internet of Things (IoT) a...</td>\n",
       "      <td>THEORETICAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>D00002</td>\n",
       "      <td>In this paper, we address the problem of compu...</td>\n",
       "      <td>THEORETICAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>D00003</td>\n",
       "      <td>High quality upsampling of sparse 3D point clo...</td>\n",
       "      <td>ENGINEERING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>D00004</td>\n",
       "      <td>Internet is the main source of information now...</td>\n",
       "      <td>EMPIRICAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>D00005</td>\n",
       "      <td>Automated Facial Expression Recognition (FER) ...</td>\n",
       "      <td>ENGINEERING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id                                           Abstract       Task 2\n",
       "0  D00001  Rapid popularity of Internet of Things (IoT) a...  THEORETICAL\n",
       "1  D00002  In this paper, we address the problem of compu...  THEORETICAL\n",
       "2  D00003  High quality upsampling of sparse 3D point clo...  ENGINEERING\n",
       "3  D00004  Internet is the main source of information now...    EMPIRICAL\n",
       "4  D00005  Automated Facial Expression Recognition (FER) ...  ENGINEERING"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainset, validset = train_test_split(dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "trainset.to_csv('trainset.csv', index=False)\n",
    "validset.to_csv('validset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../data/task2_public_testset.csv', dtype=str)\n",
    "dataset.drop('Title',axis=1,inplace=True)\n",
    "dataset.drop('Categories',axis=1,inplace=True)\n",
    "dataset.drop('Created Date',axis=1, inplace=True)\n",
    "dataset.drop('Authors',axis=1,inplace=True)\n",
    "dataset.to_csv('testset.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "PRETRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "\n",
    "NUM_LABLES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字典大小： 30522\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "\n",
    "vocab = tokenizer.vocab\n",
    "print(\"字典大小：\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(\"The\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "from multiprocessing import Pool\n",
    "        \n",
    "def sentence_to_indices(sentence, tokenizer):\n",
    "    \"\"\" Convert sentence to its word indices.\n",
    "    Args:\n",
    "        sentence (str): One string.\n",
    "    Return:\n",
    "        indices (list of int): List of word indices.\n",
    "    \"\"\"\n",
    "    return [tokenizer.convert_tokens_to_ids(word) for word in tokenizer.tokenize(sentence)]\n",
    "    \n",
    "def get_dataset(data_path, tokenizer, n_workers=4):\n",
    "    \"\"\" Load data and return dataset for training and validating.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the data.\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(data_path, dtype=str)\n",
    "\n",
    "    results = [None] * n_workers\n",
    "    with Pool(processes=n_workers) as pool:\n",
    "        for i in range(n_workers):\n",
    "            batch_start = (len(dataset) // n_workers) * i\n",
    "            if i == n_workers - 1:\n",
    "                batch_end = len(dataset)\n",
    "            else:\n",
    "                batch_end = (len(dataset) // n_workers) * (i + 1)\n",
    "            \n",
    "            batch = dataset[batch_start: batch_end]\n",
    "            results[i] = pool.apply_async(preprocess_samples, args=(batch, tokenizer))\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    processed = []\n",
    "    for result in results:\n",
    "        processed += result.get()\n",
    "    return processed\n",
    "\n",
    "def preprocess_samples(dataset, tokenizer):\n",
    "    \"\"\" Worker function.\n",
    "\n",
    "    Args:\n",
    "        dataset (list of dict)\n",
    "    Returns:\n",
    "        list of processed dict.\n",
    "    \"\"\"\n",
    "    processed = []\n",
    "    for sample in tqdm(dataset.iterrows(), total=len(dataset)):\n",
    "        processed.append(preprocess_sample(sample[1], tokenizer))\n",
    "\n",
    "    return processed\n",
    "\n",
    "def preprocess_sample(data, tokenizer):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (dict)\n",
    "    Returns:\n",
    "        dict\n",
    "    \"\"\"\n",
    "    processed = [sentence_to_indices(sent, tokenizer) for sent in data['Abstract'].split('$$$')]\n",
    "    processed = sum(processed, [])\n",
    "    \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Start processing trainset...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[INFO] Start processing validset...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[INFO] Start processing testset...\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('[INFO] Start processing trainset...')\n",
    "train = get_dataset('trainset.csv', tokenizer, n_workers=4)\n",
    "print('[INFO] Start processing validset...')\n",
    "valid = get_dataset('validset.csv', tokenizer, n_workers=4)\n",
    "print('[INFO] Start processing testset...')\n",
    "test = get_dataset('testset.csv', tokenizer, n_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "def handle_tfidf(datas):\n",
    "    vectorizer = TfidfVectorizer(tokenizer=identity_tokenizer, lowercase=False)\n",
    "    return vectorizer, vectorizer.fit_transform(datas)\n",
    "\n",
    "def get_tfidf(datas):\n",
    "    vectorizer, tfidf = handle_tfidf(datas)\n",
    "    return data_fit_tfidf(datas, tfidf.toarray(), vectorizer.vocabulary_)\n",
    "\n",
    "def data_fit_tfidf(datas, tfidf, vocab):\n",
    "    tfidfs = []\n",
    "    for idx, data in enumerate(datas):\n",
    "        word_idxs = [vocab[word] for word in data]\n",
    "        tfidfs.append([tfidf[idx, word_idx] for word_idx in word_idxs])\n",
    "    return tfidfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = get_tfidf(train + valid + test)\n",
    "train_tfidf = tfidf[:6300]\n",
    "valid_tfidf = tfidf[6300:7000]\n",
    "test_tfidf = tfidf[7000:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
