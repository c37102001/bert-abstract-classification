{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可能會有的問題\n",
    "* max_len 不要設太大\n",
    "* 要改bert傳入的東西要先讀讀他的source code\n",
    "* predict, plot還沒寫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* python >= 3.7\n",
    "* pytorch >= 1.0\n",
    "* transformers\n",
    "* pandas\n",
    "* nltk\n",
    "* numpy\n",
    "* sklearn\n",
    "* pickle\n",
    "* tqdm\n",
    "* json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe6d0143ab0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(58)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a  view of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Categories</th>\n",
       "      <th>Created Date</th>\n",
       "      <th>Task 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>D00001</td>\n",
       "      <td>A Brain-Inspired Trust Management Model to Ass...</td>\n",
       "      <td>Rapid popularity of Internet of Things (IoT) a...</td>\n",
       "      <td>Mahmud/Kaiser/Rahman/Rahman/Shabut/Al-Mamun/Hu...</td>\n",
       "      <td>cs.CR/cs.AI/q-bio.NC</td>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>THEORETICAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>D00002</td>\n",
       "      <td>On Efficient Computation of Shortest Dubins Pa...</td>\n",
       "      <td>In this paper, we address the problem of compu...</td>\n",
       "      <td>Sadeghi/Smith</td>\n",
       "      <td>cs.SY/cs.RO/math.OC</td>\n",
       "      <td>2016-09-21</td>\n",
       "      <td>THEORETICAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>D00003</td>\n",
       "      <td>Data-driven Upsampling of Point Clouds</td>\n",
       "      <td>High quality upsampling of sparse 3D point clo...</td>\n",
       "      <td>Zhang/Jiang/Yang/Yamakawa/Shimada/Kara</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>2018-07-07</td>\n",
       "      <td>ENGINEERING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>D00004</td>\n",
       "      <td>Accessibility or Usability of InteractSE? A He...</td>\n",
       "      <td>Internet is the main source of information now...</td>\n",
       "      <td>Aqle/Khowaja/Al-Thani</td>\n",
       "      <td>cs.HC</td>\n",
       "      <td>2018-08-29</td>\n",
       "      <td>EMPIRICAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>D00005</td>\n",
       "      <td>Spatio-Temporal Facial Expression Recognition ...</td>\n",
       "      <td>Automated Facial Expression Recognition (FER) ...</td>\n",
       "      <td>Hasani/Mahoor</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>2017-03-20</td>\n",
       "      <td>ENGINEERING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id                                              Title  \\\n",
       "0  D00001  A Brain-Inspired Trust Management Model to Ass...   \n",
       "1  D00002  On Efficient Computation of Shortest Dubins Pa...   \n",
       "2  D00003             Data-driven Upsampling of Point Clouds   \n",
       "3  D00004  Accessibility or Usability of InteractSE? A He...   \n",
       "4  D00005  Spatio-Temporal Facial Expression Recognition ...   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  Rapid popularity of Internet of Things (IoT) a...   \n",
       "1  In this paper, we address the problem of compu...   \n",
       "2  High quality upsampling of sparse 3D point clo...   \n",
       "3  Internet is the main source of information now...   \n",
       "4  Automated Facial Expression Recognition (FER) ...   \n",
       "\n",
       "                                             Authors            Categories  \\\n",
       "0  Mahmud/Kaiser/Rahman/Rahman/Shabut/Al-Mamun/Hu...  cs.CR/cs.AI/q-bio.NC   \n",
       "1                                      Sadeghi/Smith   cs.SY/cs.RO/math.OC   \n",
       "2             Zhang/Jiang/Yang/Yamakawa/Shimada/Kara                 cs.CV   \n",
       "3                              Aqle/Khowaja/Al-Thani                 cs.HC   \n",
       "4                                      Hasani/Mahoor                 cs.CV   \n",
       "\n",
       "  Created Date       Task 2  \n",
       "0   2018-01-11  THEORETICAL  \n",
       "1   2016-09-21  THEORETICAL  \n",
       "2   2018-07-07  ENGINEERING  \n",
       "3   2018-08-29    EMPIRICAL  \n",
       "4   2017-03-20  ENGINEERING  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('../data/task2_trainset.csv', dtype=str)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Id**: 流水號  \n",
    "**Title**: 論文標題  \n",
    "**Abstract**: 論文摘要內容, 句子間以 **$$$** 分隔  \n",
    "**Authors**: 論文作者  \n",
    "**Categories**: 論文類別  \n",
    "**Created date**: 論文上傳日期  \n",
    "**Task 2**: 論文分類類別, 若句子有多個類別,以 **空格** 分隔 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 刪除多於資訊 (Remove redundant information)  \n",
    "我們在資料集中保留了許多額外資訊供大家使用，但是在這次的教學中我們並沒有用到全部資訊，因此先將多餘的部分先抽走。  \n",
    "In dataset, we reserved lots of information. But in this tutorial, we don't need them, so we need to discard them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop('Title',axis=1,inplace=True)\n",
    "dataset.drop('Categories',axis=1,inplace=True)\n",
    "dataset.drop('Created Date',axis=1, inplace=True)\n",
    "dataset.drop('Authors',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Task 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>D00001</td>\n",
       "      <td>Rapid popularity of Internet of Things (IoT) a...</td>\n",
       "      <td>THEORETICAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>D00002</td>\n",
       "      <td>In this paper, we address the problem of compu...</td>\n",
       "      <td>THEORETICAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>D00003</td>\n",
       "      <td>High quality upsampling of sparse 3D point clo...</td>\n",
       "      <td>ENGINEERING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>D00004</td>\n",
       "      <td>Internet is the main source of information now...</td>\n",
       "      <td>EMPIRICAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>D00005</td>\n",
       "      <td>Automated Facial Expression Recognition (FER) ...</td>\n",
       "      <td>ENGINEERING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id                                           Abstract       Task 2\n",
       "0  D00001  Rapid popularity of Internet of Things (IoT) a...  THEORETICAL\n",
       "1  D00002  In this paper, we address the problem of compu...  THEORETICAL\n",
       "2  D00003  High quality upsampling of sparse 3D point clo...  ENGINEERING\n",
       "3  D00004  Internet is the main source of information now...    EMPIRICAL\n",
       "4  D00005  Automated Facial Expression Recognition (FER) ...  ENGINEERING"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料切割  (Partition)\n",
    "在訓練時，我們需要有個方法去檢驗訓練結果的好壞，因此需要將訓練資料切成training/validataion set。   \n",
    "While training, we need some method to exam our model's performance, so we divide our training data into training/validataion set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainset, validset = train_test_split(dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "trainset.to_csv('trainset.csv', index=False)\n",
    "validset.to_csv('validset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../data/task2_public_testset.csv', dtype=str)\n",
    "dataset.drop('Title',axis=1,inplace=True)\n",
    "dataset.drop('Categories',axis=1,inplace=True)\n",
    "dataset.drop('Created Date',axis=1, inplace=True)\n",
    "dataset.drop('Authors',axis=1,inplace=True)\n",
    "dataset.to_csv('testset.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料格式化 (Data formatting)  \n",
    "有了字典後，接下來我們要把資料整理成一筆一筆，把input的句子轉成數字，把答案轉成onehot的形式。  \n",
    "這裡，我們一樣使用`multiprocessing`來加入進行。  \n",
    "After building dictionary, that's mapping our sentences into number array, and convert answers to onehot format.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "PRETRAINED_MODEL_NAME = 'bert-base-cased'\n",
    "\n",
    "NUM_LABLES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字典大小： 28996\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "\n",
    "vocab = tokenizer.vocab\n",
    "print(\"字典大小：\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def label_to_onehot(labels):\n",
    "    \"\"\" Convert label to onehot .\n",
    "        Args:\n",
    "            labels (string): sentence's labels.\n",
    "        Return:\n",
    "            outputs (onehot list): sentence's onehot label.\n",
    "    \"\"\"\n",
    "    label_dict = {'THEORETICAL': 0, 'ENGINEERING':1, 'EMPIRICAL':2, 'OTHERS':3}\n",
    "    onehot = [0,0,0,0]\n",
    "    for l in labels.split():\n",
    "        onehot[label_dict[l]] = 1\n",
    "    return onehot\n",
    "        \n",
    "def sentence_to_indices(sent, tokenizer):\n",
    "    \"\"\" Convert sentence to its word indices.\n",
    "    Args:\n",
    "        sentence (str): One string.\n",
    "    Return:\n",
    "        indices (list of int): List of word indices.\n",
    "    \"\"\"\n",
    "    return [tokenizer.convert_tokens_to_ids(word) for word in tokenizer.tokenize(sent)]\n",
    "    \n",
    "def get_dataset(data_path, tokenizer, n_workers=4):\n",
    "    \"\"\" Load data and return dataset for training and validating.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the data.\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(data_path, dtype=str)\n",
    "\n",
    "    results = [None] * n_workers\n",
    "    with Pool(processes=n_workers) as pool:\n",
    "        for i in range(n_workers):\n",
    "            batch_start = (len(dataset) // n_workers) * i\n",
    "            if i == n_workers - 1:\n",
    "                batch_end = len(dataset)\n",
    "            else:\n",
    "                batch_end = (len(dataset) // n_workers) * (i + 1)\n",
    "            \n",
    "            batch = dataset[batch_start: batch_end]\n",
    "            results[i] = pool.apply_async(preprocess_samples, args=(batch, tokenizer))\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    processed = []\n",
    "    for result in results:\n",
    "        processed += result.get()\n",
    "    return processed\n",
    "\n",
    "def preprocess_samples(dataset, tokenizer):\n",
    "    \"\"\" Worker function.\n",
    "\n",
    "    Args:\n",
    "        dataset (list of dict)\n",
    "    Returns:\n",
    "        list of processed dict.\n",
    "    \"\"\"\n",
    "    processed = []\n",
    "    for sample in tqdm(dataset.iterrows(), total=len(dataset)):\n",
    "        processed.append(preprocess_sample(sample[1], tokenizer))\n",
    "\n",
    "    return processed\n",
    "\n",
    "def preprocess_sample(data, tokenizer):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (dict)\n",
    "    Returns:\n",
    "        dict\n",
    "    \"\"\"\n",
    "    processed = {}\n",
    "    processed['tokens'] = [sentence_to_indices(sent, tokenizer) \n",
    "                           for sent in data['Abstract'].split('$$$')]\n",
    "    processed['tokens'] = sum(processed['tokens'], [])\n",
    "    processed['tokens'] = [tokenizer.convert_tokens_to_ids('[CLS]')] + processed['tokens'] + [tokenizer.convert_tokens_to_ids('[SEP]')]\n",
    "    processed['segments'] = [0] * len(processed['tokens'])\n",
    "    \n",
    "    if 'Task 2' in data:\n",
    "        processed['Label'] = label_to_onehot(data['Task 2'])\n",
    "        \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Start processing trainset...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[INFO] Start processing validset...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[INFO] Start processing testset...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('[INFO] Start processing trainset...')\n",
    "train = get_dataset('trainset.csv', tokenizer, n_workers=8)\n",
    "print('[INFO] Start processing validset...')\n",
    "valid = get_dataset('validset.csv', tokenizer, n_workers=8)\n",
    "print('[INFO] Start processing testset...')\n",
    "test = get_dataset('testset.csv', tokenizer, n_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "711\n"
     ]
    }
   ],
   "source": [
    "# max_len sentence\n",
    "m = 0\n",
    "for t in test:\n",
    "    if len(t['tokens']) > m:\n",
    "        m = len(t['tokens'])\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料封裝 (Data packing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "為了更方便的進行batch training，我們將會借助[torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)。  \n",
    "而要將資料放入dataloader，我們需要繼承[torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)，撰寫適合這份dataset的class。  \n",
    "`collate_fn`用於batch data的後處理，在`dataloder`將選出的data放進list後會呼叫collate_fn，而我們會在此把sentence padding到同樣的長度，才能夠放入torch tensor (tensor必須為矩陣)。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To easily training in batch, we'll use `dataloader`, which is a function built in Pytorch[torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)  \n",
    "To use datalaoder, we need to packing our data into class `dataset` [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)  \n",
    "`collate_fn` is used for data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class BertDataset(Dataset):\n",
    "    def __init__(self, data, max_len = 512):\n",
    "        self.data = data\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "        \n",
    "    def collate_fn(self, datas):\n",
    "        # get max length in this batch\n",
    "        max_len = max([min(len(data['tokens']), self.max_len) for data in datas])\n",
    "        batch_tokens = []\n",
    "        batch_segments = []\n",
    "        batch_masks = []\n",
    "        batch_label = []\n",
    "        for data in datas:\n",
    "            # padding abstract to make them in same length\n",
    "            abstract_len = len(data['tokens'])\n",
    "            if abstract_len > max_len:\n",
    "                batch_tokens.append(data['tokens'][:max_len])\n",
    "                batch_segments.append(data['segments'][:max_len])\n",
    "                batch_masks.append([1] * max_len)\n",
    "            else:\n",
    "                batch_tokens.append(data['tokens'] + [0] * (max_len-abstract_len))\n",
    "                batch_segments.append(data['segments'] + [0] * (max_len-abstract_len))\n",
    "                batch_masks.append([1] * abstract_len  + [0] * (max_len-abstract_len))\n",
    "            # gather labels\n",
    "            if 'Label' in data:\n",
    "                batch_label.append(data['Label'])\n",
    "        return torch.LongTensor(batch_tokens), torch.LongTensor(batch_segments), torch.LongTensor(batch_masks), torch.FloatTensor(batch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = BertDataset(train)\n",
    "validData = BertDataset(valid)\n",
    "testData = BertDataset(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForMultiLabelSequenceClassification(BertPreTrainedModel):\n",
    "    \"\"\"BERT model for classification.\n",
    "    This module is composed of the BERT model with a linear layer on top of\n",
    "    the pooled output.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(BertForMultiLabelSequenceClassification, self).__init__(config)\n",
    "        \n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, \n",
    "                position_ids=None, head_mask=None, labels=None):\n",
    "        \n",
    "        outputs = self.bert(input_ids, \n",
    "                            token_type_ids = token_type_ids, \n",
    "                            attention_mask = attention_mask, \n",
    "                            position_ids=None, head_mask=None)\n",
    "        \n",
    "        pooled_output = outputs[1]\n",
    "        \n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        outputs = (logits,) + outputs[1:]\n",
    "        \n",
    "        return outputs[0]\n",
    "        \n",
    "    def freeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = BertForMultiLabelSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, num_labels = 4)\n",
    "# model.freeze_bert_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForMultiLabelSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "指定使用的運算裝置  \n",
    "Designate running device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定義一個算分公式, 讓我們在training能快速了解model的效能\n",
    "Define score function, let us easily observe model performance while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1():\n",
    "    def __init__(self):\n",
    "        self.threshold = 0.0\n",
    "        self.n_precision = 0\n",
    "        self.n_recall = 0\n",
    "        self.n_corrects = 0\n",
    "        self.name = 'F1'\n",
    "\n",
    "    def reset(self):\n",
    "        self.n_precision = 0\n",
    "        self.n_recall = 0\n",
    "        self.n_corrects = 0\n",
    "\n",
    "    def update(self, predicts, groundTruth):\n",
    "        predicts = predicts > self.threshold\n",
    "        self.n_precision += torch.sum(predicts).data.item()\n",
    "        self.n_recall += torch.sum(groundTruth).data.item()\n",
    "        self.n_corrects += torch.sum(groundTruth.type(torch.bool) * predicts).data.item()\n",
    "\n",
    "    def get_score(self):\n",
    "        recall = self.n_corrects / self.n_recall\n",
    "        precision = self.n_corrects / (self.n_precision + 1e-20)\n",
    "        return 2 * (recall * precision) / (recall + precision + 1e-20)\n",
    "\n",
    "    def print_score(self):\n",
    "        score = self.get_score()\n",
    "        return '{:.5f}'.format(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定義參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "EPOCHS = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def _run_epoch(epoch, training):\n",
    "    model.train(training)\n",
    "    if training:\n",
    "        description = 'Train'\n",
    "        BATCH_SIZE = 2\n",
    "        dataset = trainData\n",
    "        shuffle = True\n",
    "    else:\n",
    "        description = 'Valid'\n",
    "        BATCH_SIZE = 2\n",
    "        dataset = validData\n",
    "        shuffle = False\n",
    "    dataloader = DataLoader(dataset=dataset,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=shuffle,\n",
    "                            collate_fn=dataset.collate_fn,\n",
    "                            num_workers=4)\n",
    "    \n",
    "    trange = tqdm(enumerate(dataloader), total=len(dataloader), desc=description)\n",
    "    loss = 0.0\n",
    "    f1_score = F1()\n",
    "    for i, (tokens, segments, masks, labels) in trange:\n",
    "        o_labels, batch_loss = _run_iter(tokens, segments, masks, labels)\n",
    "        \n",
    "        if training:\n",
    "            opt.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            opt.step()\n",
    "        \n",
    "        \n",
    "        loss += batch_loss.item()\n",
    "        f1_score.update(o_labels.cpu(), labels)\n",
    "        \n",
    "        trange.set_postfix(\n",
    "            loss=loss / (i + 1), f1=f1_score.print_score())\n",
    "    if training:\n",
    "        history['train'].append({'f1':f1_score.get_score(), 'loss':loss/ len(trange)})\n",
    "    else:\n",
    "        history['valid'].append({'f1':f1_score.get_score(), 'loss':loss/ len(trange)})\n",
    "\n",
    "        \n",
    "def _run_iter(tokens, segments, masks, labels):\n",
    "    tokens = tokens.to(device)\n",
    "    segments = segments.to(device)\n",
    "    masks = masks.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model(tokens, token_type_ids = segments, attention_mask=masks)\n",
    "    l_loss = criteria(outputs, labels)\n",
    "    return outputs, l_loss\n",
    "\n",
    "def save(epoch):\n",
    "    if not os.path.exists('model'):\n",
    "        os.makedirs('model')\n",
    "    torch.save(model.state_dict(), 'model/model-1.pkl.'+str(epoch))\n",
    "    with open('model/history-1.json', 'w') as f:\n",
    "        json.dump(history, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange\n",
    "import json\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-5, eps = 1e-8)\n",
    "\n",
    "criteria = torch.nn.BCEWithLogitsLoss()\n",
    "model.to(device)\n",
    "history = {'train':[],'valid':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f52081bebf94b9d8bb0c4f97483a257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train', max=3150, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b581ed298f8e42a1ab2752fe8e679a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Valid', max=350, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3da22119d54224b703515d3b7a4b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train', max=3150, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453a08901ff8423584fe0b87b8f225db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Valid', max=350, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a7d2990a9b344d4b06ba1903c95e480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train', max=3150, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    if epoch > 1:\n",
    "        model.freeze_bert_encoder()\n",
    "    _run_epoch(epoch, True)\n",
    "    _run_epoch(epoch, False)\n",
    "    save(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "with open('model/history-1.json', 'r') as f:\n",
    "    history = json.loads(f.read())\n",
    "    \n",
    "train_loss = [l['loss'] for l in history['train']]\n",
    "valid_loss = [l['loss'] for l in history['valid']]\n",
    "train_f1 = [l['f1'] for l in history['train']]\n",
    "valid_f1 = [l['f1'] for l in history['valid']]\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.title('Loss')\n",
    "plt.plot(train_loss, label='train')\n",
    "plt.plot(valid_loss, label='valid')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.title('F1 Score')\n",
    "plt.plot(train_f1, label='train')\n",
    "plt.plot(valid_f1, label='valid')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Best F1 score ', max([[l['f1'], idx] for idx, l in enumerate(history['valid'])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model/model-1.pkl.{}'.format(2)))\n",
    "model.train(False)\n",
    "# _run_epoch(1, False)\n",
    "dataloader = DataLoader(dataset=testData,\n",
    "                            batch_size=16,\n",
    "                            shuffle=False,\n",
    "                            collate_fn=testData.collate_fn,\n",
    "                            num_workers=4)\n",
    "trange = tqdm(enumerate(dataloader), total=len(dataloader), desc='Predict')\n",
    "prediction = []\n",
    "for i, (tokens, segments, masks, labels) in trange:\n",
    "    with torch.no_grad():\n",
    "        o_labels = model(tokens.to(device), segments.to(device), masks.to(device))\n",
    "        o_labels = o_labels>0.0\n",
    "        prediction.append(o_labels.to('cpu'))\n",
    "\n",
    "prediction = torch.cat(prediction).detach().numpy().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SubmitGenerator(prediction, sampleFile, public=True, filename='prediction-1.csv'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        prediction (numpy array)\n",
    "        sampleFile (str)\n",
    "        public (boolean)\n",
    "        filename (str)\n",
    "    \"\"\"\n",
    "    sample = pd.read_csv(sampleFile)\n",
    "    submit = {}\n",
    "    submit['order_id'] = list(sample.order_id.values)\n",
    "    redundant = len(sample) - prediction.shape[0]\n",
    "    if public:\n",
    "        submit['THEORETICAL'] = list(prediction[:,0]) + [0]*redundant\n",
    "        submit['ENGINEERING'] = list(prediction[:,1]) + [0]*redundant\n",
    "        submit['EMPIRICAL'] = list(prediction[:,2]) + [0]*redundant\n",
    "        submit['OTHERS'] = list(prediction[:,3]) + [0]*redundant\n",
    "    else:\n",
    "        submit['THEORETICAL'] = [0]*redundant + list(prediction[:,0])\n",
    "        submit['ENGINEERING'] = [0]*redundant + list(prediction[:,1])\n",
    "        submit['EMPIRICAL'] = [0]*redundant + list(prediction[:,2])\n",
    "        submit['OTHERS'] = [0]*redundant + list(prediction[:,3])\n",
    "    df = pd.DataFrame.from_dict(submit) \n",
    "    df.to_csv(filename,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
